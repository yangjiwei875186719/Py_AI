 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。
 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。
 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。 本文回顾中文分词在2007-2017十年间的技术进展，尤其是自深度学习渗透到自然语言处理以来的
主要工作。我们的基本结论是，中文分词的监督机器学习方法在从非神经网络方法到神经网络方法的
迁移中尚未展示出明显的技术优势。中文分词的机器学习模型的构建，依然需要平衡考虑已知词和未
登录词的识别问题。尽管迄今为止深度学习应用于中文分词尚未能全面超越传统的机器学习方法，我
们审慎推测，由于人工智能联结主义基础下的神经网络模型有潜力契合自然语言的内在结构分解方式，
从而有效建模，或能在不远将来展示新的技术进步成果。
中文分词是中文信息处理的一个基础任务和研究方向。十年前，黄和赵(2007)接受《中文信息学报》
委托，针对自20世纪末以来的中文分词的机器学习方法做了十年回顾，发表了《中文分词十年回顾》一
文。该文的基本结论是中文分词的统计机器学习方法优于传统的规则方法，尤其在未登录词
语料准备之外，还有两个历史性的因素，迟滞了中文分词这一中文信息处理基础子任务走向彻底的
机器学习。其一，长期以来，中文分词的经典方法，即最大匹配算法，在合适的词典搭配下，通常能够
取得一定程度上颇可接受的性能。以F值度量，最大匹配分词一般情况下约能获得80%甚至更高的成绩。
这类简单有效的规则方法的存在，极大降低了研发先进机器学习技术的迫切性。其二，机器学习方法的
计算代价巨大，在必要硬件条件尚未普及或者成本仍过于高昂的情况下，机器学习方法的优势得不到体
现。2005年，典型的分词学习工具条件随机场（conditional random field, CRF）在百万词语料库上的训练，
需要12-18小时的单线程CPU时间，占用内存2-3G，远超当时个人计算机的一般硬件配置水准。
因此，我们在2017年的今天回顾10年前的学术状态，必须历史性地考虑当时当地的具体情形，才能
理解当时以及后来那些理所当然的技术进步有其内在而特殊的合理性和必然性。最近10年机器学习领域
最为显著的技术井喷，显然是深度学习方法的崛起和全面覆盖。因而，我们下面将技术总结分为两大部
分，即中文分词的传统机器学习模型和最近的深度学习（神经网络）模型。本文对技术论文的最新引用
截止至ACL-2017会议的部分已录用论文。但由于篇幅所限，我们仅关注严格意义上的监督机器学习模式
下的相关工作，而对于非监督学习、半监督学习、领域迁移学习以及其它分词方法和应用等，则尚付阙
如，有待日后或各路高贤的努力。我们冒昧以此相对狭窄的视角，回顾我们力所能及范围内的一些当时
及当今相对前沿的研究工作，旨在抛砖引玉，以供借鉴。
基于子串的直接标注模型事实上过强地应用了已知词信息，因为所有子串都属于已知词，并且在模
型一开始就不能再切分。这一缺陷后来得到修正，主要的工作包括Zhao & Kit (2007b; 2008b; 2008a; 
2011)。在这些工作中，对已有工作的改进主要有两点：其一，所有可能子串按照某个特定的统计度量方
式根据训练集上的n-gram计数来进行打分；其二，基本模型还是字位标注学习，前面获得的子串信息以
附加特征形式出现。这一工作获得了传统标注模型下的最佳性能，包括囊括2008年SIGHAN Bakeoff-4的
全部五项分词封闭测试的第一名(Zhao & Kit, 2008b)。当子串的抽取和统计度量得分计算扩展到训练集之
外，Zhao & Kit (2011)实际上提出了一种扩展性很强的半监督分词方法，实验也验证了其有效性。
和以上所有基于串标注，无论是线性链CRF标注还是semi-CRF标注的方法都不相同，Zhang & Clark 
(2007)引入了一种基于整句切分结构学习的分词方法。虽然他们声称这是一种基于词的方法，但是他们
的方法不同于以往的最显著点，是字和词的n-gram特征，都以同等地位在整句的切分结构分解中进行特
征提取。在细节上，他们采用了扩展的感知机算法进行训练，在解码阶段则使用近似的定宽搜索（beam 
search）。尽管其模型具备理论上更广泛的特征表达能力，但事实上该工作未能给出更佳的分词性能（参
见表6的结果对比）。