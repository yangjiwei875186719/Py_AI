# -*- coding: utf-8 -*-
import torch
x = [1,2,3,4]
x_1 = [[1,2,3,4],[2,3,5,6]]
# torchå®ç°çš„softmax
"""
ç¬¬äºŒä¸ªå‚æ•°æ˜¯ dim å‚æ•°ï¼Œç”¨äºæŒ‡å®šåœ¨å“ªä¸ªç»´åº¦ä¸Šè¿›è¡Œ softmax æ“ä½œã€‚
å½“ dim=0 æ—¶ï¼Œè¡¨ç¤ºåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œ softmax æ“ä½œã€‚è¿™é€šå¸¸ç”¨äºå¯¹æ¯ä¸€åˆ—è¿›è¡Œ softmax æ“ä½œï¼Œå³åœ¨è¾“å…¥å¼ é‡çš„åˆ—ç»´åº¦ä¸Šåº”ç”¨ softmax å‡½æ•°
å½“ dim=1 æ—¶ï¼Œè¡¨ç¤ºåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œ softmax æ“ä½œã€‚è¿™é€šå¸¸ç”¨äºå¯¹æ¯ä¸€è¡Œè¿›è¡Œ softmax æ“ä½œï¼Œå³åœ¨è¾“å…¥å¼ é‡çš„åˆ—ç»´åº¦ä¸Šåº”ç”¨ softmax å‡½æ•°
"""
print(torch.softmax(torch.Tensor(x),0))
print(torch.softmax(torch.Tensor(x),0))
print("ç«–å‘softmax:",torch.softmax(torch.Tensor(x_1),0))
print("æ¨ªå‘softmax:",torch.softmax(torch.Tensor(x_1),1))

h = torch.rand(2,3,4)
h= torch.tril(h, diagonal=-1)
print("h",h)

print("torch.Tensor(x)", torch.Tensor(x))
print("torch.FloatTensor(x)", torch.FloatTensor(x))

"""
argmax è¿”å›æœ€å¤§å¼ é‡çš„ç´¢å¼•ä½ç½®
"""
arg_max = torch.argmax(torch.Tensor(x))
print("arg_max", arg_max)


"""
Tensorå’ŒNumPyç›¸äº’è½¬æ¢
"""
a = torch.ones(5)
b = a.numpy()
print(a, b)

a += 1
print(a, b)
b += 1
print(a, b)
"""
NumPyæ•°ç»„è½¬Tensor
"""
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
print(a, b)

a += 1
print(a, b)
b += 1
print(a, b)

"""
Tensor on GPU
"""
# ä»¥ä¸‹ä»£ç åªæœ‰åœ¨PyTorch GPUç‰ˆæœ¬ä¸Šæ‰ä¼šæ‰§è¡Œ
if torch.cuda.is_available():
    device = torch.device("cuda")          # GPU
    y = torch.ones_like(x, device=device)  # ç›´æ¥åˆ›å»ºä¸€ä¸ªåœ¨GPUä¸Šçš„Tensor
    x = x.to(device)                       # ç­‰ä»·äº .to("cuda")
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # to()è¿˜å¯ä»¥åŒæ—¶æ›´æ”¹æ•°æ®ç±»å‹

"""
ones(*sizes)	å…¨1Tensor
zeros(*sizes)	å…¨0Tensor
"""

"""
viewä½¿ç”¨
view(-1)çš„ä½œç”¨æ˜¯å°†å¼ é‡é‡æ–°reshapeä¸ºä¸€ä¸ªç»´åº¦æœªçŸ¥çš„å½¢çŠ¶ï¼Œå…¶ä¸­-1è¡¨ç¤ºPyTorchä¼šæ ¹æ®å¼ é‡çš„æ€»å…ƒç´ æ•°é‡å’Œå…¶ä»–ç»´åº¦çš„å¤§å°æ¥è‡ªåŠ¨ç¡®å®šè¿™ä¸ªæœªçŸ¥ç»´åº¦çš„å¤§å°
é€šå¸¸ç”¨äºå°†ä¸€ä¸ªå¤šç»´å¼ é‡é‡æ–°æ•´å½¢ä¸ºä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œæˆ–è€…åœ¨ä¸çŸ¥é“å…·ä½“ç»´åº¦å¤§å°çš„æƒ…å†µä¸‹ï¼Œè®©PyTorchè‡ªåŠ¨è®¡ç®—è¿™ä¸ªç»´åº¦çš„å¤§å°
"""
x = torch.rand(5, 3)
print("éšæœºå¼ é‡ï¼š",x)  # å½¢çŠ¶5*3
y = x.view(-1)
print("è½¬æ¢å½¢çŠ¶ï¼š",y)   # å½¢çŠ¶1*15  é¡ºåºä¸å˜
y = x.view(-1,3) # å°†å¤šç»´å‘é‡è½¬åŒ–æˆä¸€ä½å‘é‡ ï¼Œä¾‹å¦‚ä¸€ç»´å‘é‡15ä¸ªå€¼ï¼Œåé¢ä¸€ä¸ª3 å°±å½¢æˆäº†ä¸€ä¸ª5* 3 çš„å‘é‡
print("è½¬æ¢å½¢çŠ¶ï¼š",y)  # å½¢çŠ¶ 5 * 3  é¡ºåºä¸å˜
z = x.T              # å½¢çŠ¶ 5 * 3 ç«–å˜è¡Œï¼Œè¡Œå˜ç«–
print("è½¬ç½®ï¼š",z)

x = torch.rand(5, 3,2)
print("éšæœºå¼ é‡ï¼š",x)  # 5*3*2
y = x.view(-1)
print("è½¬æ¢å½¢çŠ¶ï¼š",y)  # 1*30
y = x.view(-1,3,5) # å°†å¤šç»´å‘é‡è½¬åŒ–æˆä¸€ä½å‘é‡ ï¼Œåœ¨ç”¨30/3/5 å¾—åˆ°ä¸€ä¸ª 2*3*5çš„å¼ é‡
print("è½¬æ¢å½¢çŠ¶ï¼š",y) # 2*3*5

"""
åœ¨PyTorchä¸­ï¼Œå¯¹äºé«˜äº2ç»´çš„å¼ é‡ï¼Œç›´æ¥ä½¿ç”¨.Tè¿›è¡Œè½¬ç½®æ“ä½œå·²è¢«å¼ƒç”¨ï¼Œå¹¶åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­ä¼šå¯¼è‡´é”™è¯¯ã€‚æ›¿ä»£æ–¹æ³•æ˜¯ä½¿ç”¨.permute()ï¼Œ.transpose()æˆ–è€….mTç­‰æ“ä½œæ¥å®ç°ç›¸åŒçš„åŠŸèƒ½ã€‚
"""
x = torch.rand(5, 3, 2)
# z = x.T # è½¬ç½®æŠ¥é”™
# print("è½¬ç½®",z)
z = x.permute(0, 2, 1)  # è½¬ç½®æ“ä½œ
print("z_permute",z)  # 5*2*3
# æˆ–è€…ä½¿ç”¨ transpose
z = x.transpose(0, 2)
print("z_transpose",z)  # 2*3*5

y = torch.rand(2, 3)
z = x.view(-1,y.shape[-1])
print("y.shape[-1]",y.shape[-1])   # y.shape[-1]å°±æ˜¯å»yå½¢çŠ¶ï¼Œæœ€ä¼šä¸€ä½
print("z",z)



# attention_mask = torch.tril(10)
# print("attention_mask:",attention_mask)




"""
åŠ è½½æ¨¡å‹
åœ¨ğŸ¤— Transformersåº“ä¸­ï¼Œreturn_dictæ˜¯from_pretrainedæ–¹æ³•ä¸­çš„ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºæ§åˆ¶æ˜¯å¦è¿”å›ä¸€ä¸ªå­—å…¸å¯¹è±¡è€Œä¸æ˜¯å¤šä¸ªè¾“å‡ºã€‚å½“return_dict=Trueæ—¶ï¼Œæ¨¡å‹çš„è¾“å‡ºä»¥å­—å…¸çš„å½¢å¼è¿”å›ï¼Œå…¶ä¸­åŒ…å«æ¨¡å‹çš„æ‰€æœ‰è¾“å‡ºã€‚è¿™ä¸ªå‚æ•°é€šå¸¸ç”¨äºæ–¹ä¾¿åœ°è®¿é—®æ¨¡å‹è¾“å‡ºçš„ä¸åŒéƒ¨åˆ†ï¼Œè€Œä¸å¿…é€šè¿‡ç´¢å¼•æ¥è·å–ã€‚
å½“return_dict=Falseæ—¶ï¼Œæ¨¡å‹å°†è¿”å›ä¸€ä¸ªåŒ…å«å¤šä¸ªè¾“å‡ºå…ƒç´ çš„å…ƒç»„ï¼Œé€šå¸¸æŒ‰ç…§æ¨¡å‹çš„è¾“å‡ºé¡ºåºæ’åˆ—ã€‚è¿™ç§æ–¹å¼å¯èƒ½éœ€è¦é¢å¤–çš„ç´¢å¼•æ“ä½œæ¥è·å–æ‰€éœ€çš„è¾“å‡ºã€‚
ä½¿ç”¨return_dict=Trueæ—¶ï¼Œå¯ä»¥é€šè¿‡å­—å…¸é”®æ¥è®¿é—®ä¸åŒçš„æ¨¡å‹è¾“å‡ºï¼Œè¿™æ ·å¯ä»¥æ›´å®¹æ˜“åœ°ç†è§£å’Œå¤„ç†æ¨¡å‹çš„è¾“å‡ºç»“æœã€‚è¿™å¯¹äºåœ¨è®­ç»ƒã€è¯„ä¼°æˆ–æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦åŒæ—¶å¤„ç†å¤šä¸ªè¾“å‡ºçš„æƒ…å†µç‰¹åˆ«æœ‰å¸®åŠ©ã€‚
"""
from transformers import BertModel,BertTokenizer
from config import Config
bert = BertModel.from_pretrained(Config["bert_path"], return_dict=False)
# print("bert:", bert)


# åŠ è½½bertè¯è¡¨
"""
text:
ç±»å‹: Union[TextInput, PreTokenizedInput, EncodedInput]
æè¿°: è¾“å…¥çš„æ–‡æœ¬ï¼Œå¯ä»¥æ˜¯åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²ã€é¢„åˆ†è¯åçš„æ–‡æœ¬æˆ–è€…ç¼–ç åçš„æ–‡æœ¬ã€‚
text_pair:
ç±»å‹: å¯é€‰çš„ Union[TextInput, PreTokenizedInput, EncodedInput]
æè¿°: å¯é€‰çš„ç¬¬äºŒä¸ªå¥å­ï¼Œç”¨äºå¤„ç†å¥å¯¹ä»»åŠ¡ï¼ˆä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ä¸­çš„æ–‡æœ¬å¯¹ï¼‰ã€‚
add_special_tokens:
ç±»å‹: å¸ƒå°”å€¼
æè¿°: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚[CLS]å’Œ[SEP]ï¼‰åˆ°ç¼–ç åºåˆ—ä¸­ã€‚
padding:
ç±»å‹: Union[bool, str, PaddingStrategy]
æè¿°: æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œå¡«å……ï¼Œå¯ä»¥æ˜¯å¸ƒå°”å€¼æˆ–å¡«å……ç­–ç•¥ï¼Œç”¨äºç¡®ä¿è¾“å…¥åºåˆ—è¾¾åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ã€‚
truncation:
ç±»å‹: Union[bool, str, TruncationStrategy]
æè¿°: æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œæˆªæ–­ï¼Œå¯ä»¥æ˜¯å¸ƒå°”å€¼æˆ–æˆªæ–­ç­–ç•¥ï¼Œç”¨äºå¤„ç†è¶…è¿‡æœ€å¤§é•¿åº¦çš„è¾“å…¥ã€‚
max_length:
ç±»å‹: å¯é€‰çš„æ•´æ•°
æè¿°: æŒ‡å®šå¤„ç†åçš„æ–‡æœ¬åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œé€šå¸¸ç”¨äºæˆªæ–­å’Œå¡«å……æ“ä½œã€‚
stride:
ç±»å‹: æ•´æ•°
æè¿°: åœ¨æˆªæ–­æ—¶ï¼ŒæŒ‡å®šæˆªæ–­çª—å£çš„æ»‘åŠ¨è·ç¦»ã€‚
return_tensors:
ç±»å‹: å¯é€‰çš„å­—ç¬¦ä¸²æˆ–å¼ é‡ç±»å‹
æè¿°: æŒ‡å®šè¿”å›çš„å¼ é‡ç±»å‹ï¼Œå¦‚'pt'è¡¨ç¤ºè¿”å›PyTorchå¼ é‡ã€‚
**kwargs:
ç±»å‹: å…³é”®å­—å‚æ•°
æè¿°: å…¶ä½™æœªåˆ—å‡ºçš„å…³é”®å­—å‚æ•°å°†è¢«ä¼ é€’ç»™å†…éƒ¨è°ƒç”¨çš„æ–¹æ³•ã€‚


ç”¨åˆ°çš„å‚æ•°
add_special_tokens=False:å»æ‰å‰ç¼€
max_length=Config["max_length"]:æˆªå–æ–‡æœ¬æœ€å¤§é•¿åº¦
truncation=True:å‚æ•°å‘Šè¯‰tokenizer.encodeæ–¹æ³•åœ¨å¤„ç†è¾“å…¥æ–‡æœ¬æ—¶è¦è¿›è¡Œæˆªæ–­æ“ä½œ
padding="max_length:padding='max_length'å‚æ•°å‘Šè¯‰tokenizer.encodeæ–¹æ³•åœ¨å¤„ç†è¾“å…¥æ–‡æœ¬æ—¶è¦è¿›è¡Œå¡«å……æ“ä½œï¼Œå¹¶ä¸”å¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿è¾“å…¥åºåˆ—è¾¾åˆ°æŒ‡å®šçš„é•¿åº¦ï¼Œä»¥ç¬¦åˆBERTæ¨¡å‹å¯¹è¾“å…¥åºåˆ—é•¿åº¦çš„è¦æ±‚
"""

def load_vocab(vocab_path):
    return BertTokenizer.from_pretrained(vocab_path)
tokenizer = load_vocab(Config["bert_path"])  # è¿™ä¸ªæ˜¯bertè·¯å¾„ï¼Œä¸æ˜¯bertçš„å­—å…¸è·¯å¾„
print("tokenizer:",tokenizer)

vocab_index = tokenizer.encode("ä½ å¥½å‘€ï¼",padding="max_length",max_length=Config["max_length"],truncation=True,) ##  ï¼Œ
print("vocab_index:",vocab_index)
print(f"vocab_index:{vocab_index}ç»“æŸ")